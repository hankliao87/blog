var tipuesearch = {"pages":[{"title":"Fix Dark Text on Dark Background of QT Apps in Gnome with Dark Theme","text":"Problem When we change the theme of the QT apps to the dark varient, the text of the interference might be difficult to see on the dark background. Dark text on dark background (Goldendict) Solution Install Tools: $ yay -S kvantum-qt5 qt5ct qt5-styleplugins Add the content below in ~/.profile : export QT_QPA_PLATFORMTHEME = gtk2 export QT_STYLE_OVERRIDE = kvantum Select preferred dark theme in kvantum. Select preferred dark theme Change kvantum to kvantum-dark in the QT5 Setting as below: Change kvantum to kvantum-dark Note Sometimes you will need to logout and login again to apply the new settings. Result Interface after fix (Goldendict)","tags":"Tutorial","url":"https://hankliao87.github.io/blog/posts/2021/07/fix-dark-text-on-dark-background-of-qt-apps-in-gnome-with-dark-theme/","loc":"https://hankliao87.github.io/blog/posts/2021/07/fix-dark-text-on-dark-background-of-qt-apps-in-gnome-with-dark-theme/"},{"title":"Deploy Model on Android Device using TVM","text":"The schematic diagram of the result. The cat image is downloaded from here . Build TVM Docker Container Environment Build the TVM Docker container to ensure we has the same environment. (You can skip this section if you know how to install the dependent package and tvm4j. And you are familiar with the hierarchy of the folder of the tvm.) Install Docker. https://docs.docker.com/install/ Clone the TVM repo. $ git clone --depth 1 https://github.com/apache/incubator-tvm.git tvm Build the Docker image using the Dockerfile Dockerfile.demo_android in the folder tvm/docker . $ cd tvm/docker/ $ bash ./build.sh demo_android -it bash Exit from the temp container using ctrl+D . Build the TVM Docker container and attach it. $ docker run -it --name tvm tvm.demo_android $ docker start tvm && docker attach tvm Install tvm4j. $ apt install maven $ cd /usr/tvm/ $ make jvmdkg $ make jvminstall Test the Model Running Well on TVM Copy the onnx into the Docker container using docker cp . Install onnx. $ pip3 install onnx Run the script below to test the model. import onnx import numpy as np import tvm import tvm.relay as relay from tvm.contrib import graph_runtime # Change this to match the input of your model. input = np . ones ([ 1 , 3 , 256 , 256 ]) # Change this to match the filename of your model. onnx_model = onnx . load ( 'model.onnx' ) # Change this to match the shape of input of your model. x = np . ones (( 1 , 3 , 256 , 256 )) # Change this to match the input name of your model. input_name = 'input.1' target = 'llvm' shape_dict = { input_name : x . shape } sym , params = relay . frontend . from_onnx ( onnx_model , shape_dict ) ctx = tvm . context ( target , 0 ) with relay . build_config ( opt_level = 0 ): intrp = relay . build_module . create_executor ( 'graph' , sym , ctx , target ) with relay . build_config ( opt_level = 2 ): graph , lib , params = relay . build_module . build ( sym , target , params = params ) dtype = np . float32 module = graph_runtime . create ( graph , lib , ctx ) module . set_input ( ** params ) module . set_input ( input_name , tvm . nd . array ( input . astype ( dtype ))) module . run () output = module . get_output ( 0 ) . asnumpy () # May change this to match the output type of your model. print ( output ) Cross-compile the Model Run the script below and you will get three files ( model.so , model.json , model.params ). import onnx import numpy as np import tvm import tvm.relay as relay # Change this to match the filename of your model. onnx_model = onnx . load ( 'model.onnx' ) # Change this to match the shape of input of your model. x = np . ones (( 1 , 3 , 256 , 256 )) # Change this to match the input name of your model. input_name = 'input.1' arch = 'arm64' target = 'llvm -target= %s -linux-android' % arch shape_dict = { input_name : x . shape } sym , params = relay . frontend . from_onnx ( onnx_model , shape = shape_dict ) with relay . build_config ( opt_level = 0 ): intrp = relay . build_module . create_executor ( 'graph' , sym , tvm . cpu ( 0 ), target ) with relay . build_config ( opt_level = 2 ): graph , lib , params = relay . build_module . build ( sym , target , params = params ) libpath = 'model.so' # Change the parameter `cc` to match the architecture of your phone. # You can run `adb shell cat /proc/cpuinfo` to list the info of your CPU. # This is for Android SDK 28 (Pie) and CPU is aarch64. lib . export_library ( libpath , cc = '/opt/android-sdk-linux/ndk-bundle/toolchains/llvm/prebuilt/linux-x86_64/bin/aarch64-linux-android28-clang' ) graph_json_path = 'model.json' with open ( graph_json_path , 'w' ) as fo : fo . write ( graph ) param_path = 'model.params' with open ( param_path , 'wb' ) as fo : fo . write ( relay . save_param_dict ( params )) Write the Android Program In the folder tvm/apps/android_deploy , you will see an example provided by TVM. You can compile the Android program first to know what each functions does, or you can modified the files according to README.md Moreover, here is an Android program that I deployed the style transfer models which were trained by Tony Tseng . Compile the Android Program Change directory to the root of the android program. $ cd /usr/tvm/apps/android_deploy Generate the apk file. $ gradle clean build --no-daemon Create the key which is used to sign apk if you don't have. $ bash ./dev_tools/gen_keystore.sh Sign the apk file. $ bash ./dev_tools/sign_apk.sh The signed apk file will be ./app/build/outputs/apk/release/tvmdemo-release.apk Copy the apk file from the Docker container.","tags":"Tutorial","url":"https://hankliao87.github.io/blog/posts/2020/02/deploy-model-on-android-device-using-tvm/","loc":"https://hankliao87.github.io/blog/posts/2020/02/deploy-model-on-android-device-using-tvm/"},{"title":"Add Opencc Support for Goldendict in Archlinux","text":"Problem When you install goldendict using yay , you won't see the Chinese Conversion section in the transliteration option. This is because the method of building goldendict provided by the community doesn't add the dependent package of the chinese conversion. According to the README of goldendict , we just need to add the dependent package opencc before building it. The transliteration option in goldendict with opencc support. Solution First, download PKGBUILD and goldendict.changelog : $ yay -G goldendict After that, edit the PKGBUILD file in the folder goldendict . ... - depends=('hunspell' 'libxtst' 'libzip' 'libao' 'qt5-webkit' 'qt5-svg' 'qt5-x11extras' 'qt5-tools' 'phonon-qt5' 'ffmpeg') + depends=('hunspell' 'libxtst' 'libzip' 'libao' 'qt5-webkit' 'qt5-svg' 'qt5-x11extras' 'qt5-tools' 'phonon-qt5' 'ffmpeg' 'opencc') ... build(){ cd \"${srcdir}\"/$pkgname-1.5.0-RC2 - qmake-qt5 \"CONFIG+=no_epwing_support\" PREFIX=\"/usr\" + qmake-qt5 \"CONFIG+=no_epwing_support chinese_conversion_support\" PREFIX=\"/usr\" make } ... In the end, build and install goldendict: $ makepkg -si","tags":"Tutorial","url":"https://hankliao87.github.io/blog/posts/2020/01/add-opencc-support-for-goldendict-in-archlinux/","loc":"https://hankliao87.github.io/blog/posts/2020/01/add-opencc-support-for-goldendict-in-archlinux/"}]};